# Настройка подключения к ВМ
ssh -i ~/.ssh/yandex_cloud_new ubuntu@89.169.130.99 # для запуска подключения к виртуалке

## Создание ВМ, регистри и кластера
terraform init
terraform plan
terraform apply
terraform output external_ip # для получения ip виртуалки
# после апплая
terraform output -raw vm_public_ip
ssh -i ~/.ssh/yandex_cloud ubuntu@$(terraform output -raw vm_public_ip)



## Для обновления registry-id
# Полностью выйти из всех registry
docker logout
# Удалить все образы, связанные со старым registry
docker images | grep 'старый-id-реестра' | awk '{print $3}' | xargs docker rmi -f
# Авторизоваться заново
yc container registry configure-docker



## Абстрактные инструкции
yc iam create-token  -  создание токена
# nat_ip_address     = yandex_vpc_address.static_ip.external_ipv4_address[0].address  -  добавить в network_interface {}

yc container registry configure-docker # Авторизация в Container Registry



## Передача образов в регистри
./docker-build-and-push.sh


## Запуск подов
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/mysql.yaml
kubectl apply -f k8s/backend.yaml
kubectl apply -f k8s/frontend.yaml


## Перезагрузка пода
kubectl -n task-manager rollout restart deployment/backend
kubectl -n task-manager rollout status deployment/backend


## Перезагрузка пода с imagePullPolicy: IfNotPresent через иммутабельные теги
kubectl -n task-manager set image deployment/backend backend=cr.yandex/crpvquotsnkpcsh1hs6v/task-manager-backend:<NEW_TAG> --record


## Мониторинг: установка kube-prometheus-stack (Prometheus + Grafana)
# Установка через Helm в namespace monitoring
kubectl create ns monitoring || true
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm upgrade --install kps prometheus-community/kube-prometheus-stack -n monitoring

# Применить ServiceMonitor для бэкенда (уже добавлен в k8s/backend.yaml)
kubectl apply -f k8s/backend.yaml

# Доступ к Grafana
kubectl -n monitoring get svc kps-grafana
kubectl -n monitoring port-forward svc/kps-grafana 3001:80
# Логин/пароль из секрета
kubectl -n monitoring get secret kps-grafana -o jsonpath='{.data.admin-user}' | base64 -d; echo
kubectl -n monitoring get secret kps-grafana -o jsonpath='{.data.admin-password}' | base64 -d; echo


## Проверка метрик и HPA
# Проверить, что метрики собираются
kubectl -n task-manager port-forward svc/backend 3000:3000 # в другой вкладке
curl -s http://localhost:3000/metrics | head

# Проверить HPA
kubectl -n task-manager get hpa backend-hpa
kubectl -n task-manager get hpa backend-hpa -w

# Дать нагрузку (пример с hey)
# brew install hey
hey -z 60s -c 50 http://<EXTERNAL_IP_OR_LB>/api/health

# Наблюдать рост реплик бэкенда
kubectl -n task-manager get deploy backend -w


## Пересборка бэкенда после добавления метрик
# Требуется пересобрать и запушить образ из-за добавления prom-client и /metrics
cd backend && npm ci && cd -
./docker-build-and-push.sh
kubectl -n task-manager rollout restart deployment/backend
